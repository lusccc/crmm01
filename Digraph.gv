digraph {
	graph [size="40.65,40.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140176462908768 [label="
 (10, 10)" fillcolor=darkolivegreen1]
	140175811927200 [label=AddmmBackward0]
	140176557324464 -> 140175811927200
	140176463207856 [label="tabular_classifier.layers.1.bias
 (10)" fillcolor=lightblue]
	140176463207856 -> 140176557324464
	140176557324464 [label=AccumulateGrad]
	140176557325952 -> 140175811927200
	140176557325952 [label=NativeDropoutBackward0]
	140176557324752 -> 140176557325952
	140176557324752 [label=ReluBackward0]
	140176557324608 -> 140176557324752
	140176557324608 [label=NativeBatchNormBackward0]
	140176505701664 -> 140176557324608
	140176505701664 [label=AddmmBackward0]
	140176505702240 -> 140176505701664
	140176463207696 [label="tabular_classifier.layers.0.bias
 (32)" fillcolor=lightblue]
	140176463207696 -> 140176505702240
	140176505702240 [label=AccumulateGrad]
	140176505702192 -> 140176505701664
	140176505702192 [label=SigmoidBackward0]
	140176505701808 -> 140176505702192
	140176505701808 [label=AddmmBackward0]
	140176505700416 -> 140176505701808
	140176463530720 [label="
 (128)" fillcolor=lightblue]
	140176463530720 -> 140176505700416
	140176505700416 [label=AccumulateGrad]
	140176505701760 -> 140176505701808
	140176505701760 [label=SigmoidBackward0]
	140176505701616 -> 140176505701760
	140176505701616 [label=AddmmBackward0]
	140176505700656 -> 140176505701616
	140176463530400 [label="
 (256)" fillcolor=lightblue]
	140176463530400 -> 140176505700656
	140176505700656 [label=AccumulateGrad]
	140176505700752 -> 140176505701616
	140176505700752 [label=SigmoidBackward0]
	140176505702624 -> 140176505700752
	140176505702624 [label=AddmmBackward0]
	140176505702432 -> 140176505702624
	140176463530080 [label="
 (128)" fillcolor=lightblue]
	140176463530080 -> 140176505702432
	140176505702432 [label=AccumulateGrad]
	140176505702768 -> 140176505702624
	140176505702768 [label=AddBackward0]
	140176505702912 -> 140176505702768
	140176505702912 [label=SigmoidBackward0]
	140176505701328 -> 140176505702912
	140176505701328 [label=AddmmBackward0]
	140176505701472 -> 140176505701328
	140176463676496 [label="
 (128)" fillcolor=lightblue]
	140176463676496 -> 140176505701472
	140176505701472 [label=AccumulateGrad]
	140176505701280 -> 140176505701328
	140176505701280 [label=SigmoidBackward0]
	140176505701136 -> 140176505701280
	140176505701136 [label=AddmmBackward0]
	140176505701184 -> 140176505701136
	140176463676976 [label="
 (256)" fillcolor=lightblue]
	140176463676976 -> 140176505701184
	140176505701184 [label=AccumulateGrad]
	140176505701232 -> 140176505701136
	140176505701232 [label=SigmoidBackward0]
	140176505700944 -> 140176505701232
	140176505700944 [label=AddmmBackward0]
	140176505703056 -> 140176505700944
	140175806426224 [label="
 (128)" fillcolor=lightblue]
	140175806426224 -> 140176505703056
	140176505703056 [label=AccumulateGrad]
	140176505703008 -> 140176505700944
	140176505703008 [label=AddmmBackward0]
	140176505703152 -> 140176505703008
	140176463207136 [label="num_mlp.layers.1.bias
 (128)" fillcolor=lightblue]
	140176463207136 -> 140176505703152
	140176505703152 [label=AccumulateGrad]
	140176505703200 -> 140176505703008
	140176505703200 [label=NativeDropoutBackward0]
	140176505703344 -> 140176505703200
	140176505703344 [label=ReluBackward0]
	140176505703536 -> 140176505703344
	140176505703536 [label=NativeBatchNormBackward0]
	140176505703632 -> 140176505703536
	140176505703632 [label=AddmmBackward0]
	140176505703824 -> 140176505703632
	140176463206976 [label="num_mlp.layers.0.bias
 (25)" fillcolor=lightblue]
	140176463206976 -> 140176505703824
	140176505703824 [label=AccumulateGrad]
	140176505703776 -> 140176505703632
	140176505703776 [label=NativeBatchNormBackward0]
	140176505703920 -> 140176505703776
	140176463530800 [label="num_bn.weight
 (25)" fillcolor=lightblue]
	140176463530800 -> 140176505703920
	140176505703920 [label=AccumulateGrad]
	140176505703968 -> 140176505703776
	140176463530960 [label="num_bn.bias
 (25)" fillcolor=lightblue]
	140176463530960 -> 140176505703968
	140176505703968 [label=AccumulateGrad]
	140176505703728 -> 140176505703632
	140176505703728 [label=TBackward0]
	140176505704112 -> 140176505703728
	140176463206896 [label="num_mlp.layers.0.weight
 (25, 25)" fillcolor=lightblue]
	140176463206896 -> 140176505704112
	140176505704112 [label=AccumulateGrad]
	140176505703584 -> 140176505703536
	140176463207216 [label="num_mlp.bn.0.weight
 (25)" fillcolor=lightblue]
	140176463207216 -> 140176505703584
	140176505703584 [label=AccumulateGrad]
	140176505703440 -> 140176505703536
	140176463207296 [label="num_mlp.bn.0.bias
 (25)" fillcolor=lightblue]
	140176463207296 -> 140176505703440
	140176505703440 [label=AccumulateGrad]
	140176505703248 -> 140176505703008
	140176505703248 [label=TBackward0]
	140176505703680 -> 140176505703248
	140176463207056 [label="num_mlp.layers.1.weight
 (128, 25)" fillcolor=lightblue]
	140176463207056 -> 140176505703680
	140176505703680 [label=AccumulateGrad]
	140176505700896 -> 140176505700944
	140176505700896 [label=TBackward0]
	140176505704064 -> 140176505700896
	140176505704064 [label=TBackward0]
	140176505703392 -> 140176505704064
	140176557344864 [label="
 (128, 128)" fillcolor=lightblue]
	140176557344864 -> 140176505703392
	140176505703392 [label=AccumulateGrad]
	140176505701040 -> 140176505701136
	140176505701040 [label=TBackward0]
	140176505703488 -> 140176505701040
	140176505703488 [label=TBackward0]
	140176505704160 -> 140176505703488
	140176557345504 [label="
 (128, 256)" fillcolor=lightblue]
	140176557345504 -> 140176505704160
	140176505704160 [label=AccumulateGrad]
	140176505702528 -> 140176505701328
	140176505702528 [label=TBackward0]
	140176505703104 -> 140176505702528
	140176505703104 [label=TBackward0]
	140176505703296 -> 140176505703104
	140176463676576 [label="
 (256, 128)" fillcolor=lightblue]
	140176463676576 -> 140176505703296
	140176505703296 [label=AccumulateGrad]
	140176505702960 -> 140176505702768
	140176505702960 [label=SigmoidBackward0]
	140176505700848 -> 140176505702960
	140176505700848 [label=AddmmBackward0]
	140176505700992 -> 140176505700848
	140176463529760 [label="
 (128)" fillcolor=lightblue]
	140176463529760 -> 140176505700992
	140176505700992 [label=AccumulateGrad]
	140176505703872 -> 140176505700848
	140176505703872 [label=SigmoidBackward0]
	140176505701088 -> 140176505703872
	140176505701088 [label=AddmmBackward0]
	140176505704352 -> 140176505701088
	140176463529440 [label="
 (256)" fillcolor=lightblue]
	140176463529440 -> 140176505704352
	140176505704352 [label=AccumulateGrad]
	140176505704304 -> 140176505701088
	140176505704304 [label=SigmoidBackward0]
	140176462946368 -> 140176505704304
	140176462946368 [label=AddmmBackward0]
	140176462946560 -> 140176462946368
	140176463528000 [label="
 (128)" fillcolor=lightblue]
	140176463528000 -> 140176462946560
	140176462946560 [label=AccumulateGrad]
	140176462946512 -> 140176462946368
	140176462946512 [label=AddmmBackward0]
	140176462946656 -> 140176462946512
	140176463206336 [label="cat_mlp.layers.1.bias
 (128)" fillcolor=lightblue]
	140176463206336 -> 140176462946656
	140176462946656 [label=AccumulateGrad]
	140176462946704 -> 140176462946512
	140176462946704 [label=NativeDropoutBackward0]
	140176462946848 -> 140176462946704
	140176462946848 [label=ReluBackward0]
	140176462947040 -> 140176462946848
	140176462947040 [label=NativeBatchNormBackward0]
	140176462947136 -> 140176462947040
	140176462947136 [label=AddmmBackward0]
	140176462947328 -> 140176462947136
	140176463205056 [label="cat_mlp.layers.0.bias
 (265)" fillcolor=lightblue]
	140176463205056 -> 140176462947328
	140176462947328 [label=AccumulateGrad]
	140176462947280 -> 140176462947136
	140176462947280 [label=TBackward0]
	140176462947376 -> 140176462947280
	140176463204976 [label="cat_mlp.layers.0.weight
 (265, 1061)" fillcolor=lightblue]
	140176463204976 -> 140176462947376
	140176462947376 [label=AccumulateGrad]
	140176462947088 -> 140176462947040
	140176463206496 [label="cat_mlp.bn.0.weight
 (265)" fillcolor=lightblue]
	140176463206496 -> 140176462947088
	140176462947088 [label=AccumulateGrad]
	140176462946944 -> 140176462947040
	140176463206576 [label="cat_mlp.bn.0.bias
 (265)" fillcolor=lightblue]
	140176463206576 -> 140176462946944
	140176462946944 [label=AccumulateGrad]
	140176462946752 -> 140176462946512
	140176462946752 [label=TBackward0]
	140176462947184 -> 140176462946752
	140176463206256 [label="cat_mlp.layers.1.weight
 (128, 265)" fillcolor=lightblue]
	140176463206256 -> 140176462947184
	140176462947184 [label=AccumulateGrad]
	140176462946464 -> 140176462946368
	140176462946464 [label=TBackward0]
	140176462947424 -> 140176462946464
	140176462947424 [label=TBackward0]
	140176462946896 -> 140176462947424
	140176463528080 [label="
 (128, 128)" fillcolor=lightblue]
	140176463528080 -> 140176462946896
	140176462946896 [label=AccumulateGrad]
	140176505704256 -> 140176505701088
	140176505704256 [label=TBackward0]
	140176462946992 -> 140176505704256
	140176462946992 [label=TBackward0]
	140176462947520 -> 140176462946992
	140176463529200 [label="
 (128, 256)" fillcolor=lightblue]
	140176463529200 -> 140176462947520
	140176462947520 [label=AccumulateGrad]
	140176505701376 -> 140176505700848
	140176505701376 [label=TBackward0]
	140176505704400 -> 140176505701376
	140176505704400 [label=TBackward0]
	140176462946800 -> 140176505704400
	140176463529600 [label="
 (256, 128)" fillcolor=lightblue]
	140176463529600 -> 140176462946800
	140176462946800 [label=AccumulateGrad]
	140176505702480 -> 140176505702624
	140176505702480 [label=TBackward0]
	140176505704208 -> 140176505702480
	140176505704208 [label=TBackward0]
	140176505704016 -> 140176505704208
	140176463529920 [label="
 (128, 128)" fillcolor=lightblue]
	140176463529920 -> 140176505704016
	140176505704016 [label=AccumulateGrad]
	140176505702720 -> 140176505701616
	140176505702720 [label=TBackward0]
	140176505701424 -> 140176505702720
	140176505701424 [label=TBackward0]
	140176505700608 -> 140176505701424
	140176463530160 [label="
 (128, 256)" fillcolor=lightblue]
	140176463530160 -> 140176505700608
	140176505700608 [label=AccumulateGrad]
	140176505701904 -> 140176505701808
	140176505701904 [label=TBackward0]
	140176505702288 -> 140176505701904
	140176505702288 [label=TBackward0]
	140176505702816 -> 140176505702288
	140176463530560 [label="
 (256, 128)" fillcolor=lightblue]
	140176463530560 -> 140176505702816
	140176505702816 [label=AccumulateGrad]
	140176505701712 -> 140176505701664
	140176505701712 [label=TBackward0]
	140176505700704 -> 140176505701712
	140176463207616 [label="tabular_classifier.layers.0.weight
 (32, 128)" fillcolor=lightblue]
	140176463207616 -> 140176505700704
	140176505700704 [label=AccumulateGrad]
	140176505701520 -> 140176557324608
	140176463207936 [label="tabular_classifier.bn.0.weight
 (32)" fillcolor=lightblue]
	140176463207936 -> 140176505701520
	140176505701520 [label=AccumulateGrad]
	140176505701856 -> 140176557324608
	140176463208016 [label="tabular_classifier.bn.0.bias
 (32)" fillcolor=lightblue]
	140176463208016 -> 140176505701856
	140176505701856 [label=AccumulateGrad]
	140176557324896 -> 140175811927200
	140176557324896 [label=TBackward0]
	140176557325424 -> 140176557324896
	140176463207776 [label="tabular_classifier.layers.1.weight
 (10, 32)" fillcolor=lightblue]
	140176463207776 -> 140176557325424
	140176557325424 [label=AccumulateGrad]
	140175811927200 -> 140176462908768
}
